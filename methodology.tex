%The US SynLBD was released in 2010 to the Cornell SDS. The Census Bureauâ€™s Disclosure Review Board (DRB), as well as the Internal Revenue Service (IRS), classified SynLBD as public-use, but access is controlled due to concerns about the quality of the data. There are no disclosure concerns but researchers are cautioned not to trust results as if they were created by a traditional public-use file without going through the validation process. For similar reasons, the preparation of tabular data based on the synthetic data is strongly discouraged, and are not validated. Nevertheless, the synthetic data are of much easier access than the confidential data.


The Synthetic LBD is derived from the LBD as a partially synthetic database with analytic validity, by synthesizing the life-span of establishments, as well as the evolution of their employment, conditional on industry. Geography is not synthesized, but is suppressed from the released file \citep{RePEc:cen:tnotes:11-01}. The current version, is based on the Standard Industrial Classification (SIC) and extends through 2000. \citet{RePEc:cen:wpaper:14-12} describes efforts to create a new version of the Synthetic LBD, using a longer time  series (through 2010) and newer industry coding (NAICS), while also adjusting and extending the models for  improved  analytic validity and  the imputation of additional variables. In this paper, we refer to and re-use the older methodology, which we will call \SynLBD. Our emphasis is on the comparability of results obtained for a given methodology across the various applications.
  

We can currently distinguish between two methods to create synthetic data. The general approach to data synthesis is to generate a joint posterior predictive distribution of $Y|X$ where $Y$ are variables to be synthesized and $X$ are unsynthesized variables. In \SynLBD, variables are synthesized in a sequential fashion, with categorical variables being generally processed first using a variant of Dirichlet-Multinomial models. Continuous variables are then synthesized using a normal linear regression model with kennel density-based transformation \citep{WOODCOCK20094228}.\footnote{\textcite{RePEc:cen:wpaper:14-12} shift  to a Classification and Regression Trees (CART) model with Bayesian bootstrap. } \SynLBD{} is implemented in SAS\texttrademark, which is frequently used in national statistical offices.

To evaluate whether synthetic data algorithms developed in the U.S. can be adapted to generate similar synthetic data for other countries, \textcite{RePEc:cen:wpaper:14-13} implement \SynLBD{} to the German Longitudinal Business Database (GLBD). In this paper, we extend the analysis from the earlier paper, and extend the application to the Canadian context (SynLEAP). 


\subsection{Common processing}

In all countries, the underlying data provides annual measures. However, \SynLBD{} assumes a longitudinal (wide) structure of the dataset, with invariant industry (and location). In all cases, the modal industry is chosen to represent the entity's industrial activity. 

In all countries, the first and last years are occasionally problematic, and were dropped. Furthermore, in all countries, the synthesis of certain industries failed to complete. In both Canada and the US, this number is less than 10. In Canada, they account for about 7 percent of the total number of observations (Table \ref{tab:Synthesized_observations}).


\subsection{Limitations}

In both cases, we encountered various technical and data-driven limitations. In the German case, our experiments were limited to only a handful of industries, due to a combination of time and software availability factors. The results should still be considered preliminary. In both countries, as outlined in Section~\ref{sec:data}, there are subtle but potentially important definitions in the various variable definitions. Industry coding differs across all three countries, and the level of detail in each of the industry codings may affect the success and precision of the synthesis.\footnote{\textcite{StatisticsCanada1991}, when comparing the 1987 US \ac{SIC} to the 1980 Canadian \ac{SIC},  already pointed out that the degree of specialization, the organization of production, and the size of the respective markets differed. Thus, the density of establishments within each of the chosen categories is likely affect the quality of the synthesis.} 

Furthermore, due to the nature of the underlying data, entities are establishments in Germany and the US, but employers in Canada. \SynLBD{} should work on any level of entity aggregation (see \citet{RePEc:cen:wpaper:14-12} for an application to hierarchical firm data with both firm/employer and establishment level imputation). However, it may yet again affect the observed density of the data within industry-year categories, and therefore the overall comparability. 

Both the German and the Canadian data experience some level of industry coding change, which may affect the classification of some entities. 

Finally, due to a feature of \SynLBD{} that we do not fully understand, the last year of the data generally was of poor quality. For some industry-country pairs, this also happened in the first year. We dropped those observations. 

\subsection{Measuring outcomes}

In order to assess the outcomes of the experiment, we inspect analytical validity by various measures, the extent of confidentiality protection, and the utility for model development. To check analytical validity, we compare basic univariate time series between the synthetic and confidential data (employment, entity entry and exit rates, job creation and destruction rates), and the distribution of entities (firms and establishment, depending on country)  employment, and payroll across time by industry. 

To compare a more comprehensive measure of  quality of the synthetic data relative to the confidential data, we compute the $pMSE$ \parencite[propensity score mean-squared error,][]{Woo_Reiter_Oganian_Karr_2009,SnokeSlavkovic2018,Snoke_RSSA2018}: the mean-squared error of the predicted probabilities (i.e., propensity scores) for those two databases. Specifically, $pMSE$ is a metric to assess how well we are able to discern the high distributional similarity between synthetic data and confidential data. 

We follow  \textcite{SnokeSlavkovic2018} to calculate the $pMSE$. This method involved the following steps: 
\todo{This notation clashes with the earlier one. We need to consolidate notation. I've replaced "it" with "et" (entity) reserving the "i" for industry}
\begin{enumerate}
    \item Append the $n_1$ rows of the confidential database $X$ to the $n_2$ rows of the synthetic database $X^s$ to create $X^{comb}$ with $N=n_1 + n_2$ rows.
    \item Create a variable $I_{et}$ denoting membership of an observation for entity $e$ in the component databases,  $I_{et}=\{1: X^{comb}_{et} \in X^s\}$. $I_{et}$ takes on values of $1$ for the synthetic database and $0$ for the confidential database. 
    \item Fit the following model to predict $I$
    \begin{eqnarray}	
        I_{et} & = &\alpha + Emp_{et} + \lambda Pay_{et} + Age_{et}^{T}\beta + \lambda_t + \alpha_i + \epsilon_{et} \label{pMSE}
     \end{eqnarray}
     \todo{BD: I don't understand why the dependant variable has no indices?}
     \todo{JA: This is an indicator variable of $1$ for the synthetic database and $0$ for the confidential database. In this case, we need to add one more index in all variables as well.}
    where $Emp_{et}$ is  log employment  of entity $e$ in year $t$, $Pay_{et}$ is  log payroll of entity $e$ in year $t$, $Age_{et}$ is a vector of age classes of entity $e$ in year $t$, $\lambda_t$ is a year fixed effect, $\alpha_i$ is an unobserved time-invariant industry-specific effect, and $\epsilon_{et}$ is the disturbance term of entity $e$ in year $t$. 
    \item calculate the predicted probabilities, $\hat{I}_it$
    \item Compute the $pMSE=\frac{1}{N}\sum_{i=1}^N(\hat{p}_i - 0.5)^2$
\end{enumerate}
If $n_1 = n_2$, $pMSE$ = 0 means every $\hat{p}_i = 0.5$, and the two databases are distributionally indistinguishable


For a more complex assessment, we compute a dynamic panel data model of economic (employment) growth on each dataset. We then assess fit of the model in two ways. First, analytic validity --- statistical precision --- can be assessed using confidence interval overlap measures   \citep{tas2006}. 
We compute the \emph{interval overlap measure} $J_{k,m}$ for parameter $k$ in model $m$. Consider the overlap of confidence intervals $(L,U)$ for $\beta_{k,m}$ (estimated from the confidential data) and $(L^{*},U^{*})$ for $\beta_{k,m}^*$ (from the synthetic data). Let $L^{over} = \max (L,L^{*} )$ and $U^{over} = \min (U,U^{*})$. Then the average overlap in confidence intervals is
$$
J_{k,m}^{*} = \frac{1}{2} \left [ \frac{U^{over} - L^{over}}{U-L} + \frac{U^{over} - L^{over}}{U^*-L ^*}        \right ]
$$
We can also compute an overall score by  averaging $J_{k,m}^{*}$ over all  parameters. 