%The US SynLBD was released in 2010 to the Cornell SDS. The Census Bureauâ€™s Disclosure Review Board (DRB), as well as the Internal Revenue Service (IRS), classified SynLBD as public-use, but access is controlled due to concerns about the quality of the data. There are no disclosure concerns but researchers are cautioned not to trust results as if they were created by a traditional public-use file without going through the validation process. For similar reasons, the preparation of tabular data based on the synthetic data is strongly discouraged, and are not validated. Nevertheless, the synthetic data are of much easier access than the confidential data.


The Synthetic LBD is derived from the LBD as a partially synthetic database with analytic validity, by synthesizing the life-span of establishments, as well as the evolution of their employment, conditional on industry. Geography is not synthesized, but is suppressed from the released file \citep{RePEc:cen:tnotes:11-01}. The current version, is based on the Standard Industrial Classification (SIC) and extends through 2000. \citet{RePEc:cen:wpaper:14-12} describes efforts to create a new version of the Synthetic LBD, using a longer time  series (through 2010) and newer industry coding (NAICS), while also adjusting and extending the models for  improved  analytic validity and  the imputation of additional variables. In this paper, we refer to and re-use the older methodology, which we will call \SynLBD. Our emphasis is on the comparability of results obtained for a given methodology across the various applications.
  

%\deleted{We can currently distinguish between two methods to create synthetic data.}\todo{Since we do not describe the two approaches, I would suggest dropping this sentence.} 
The general approach to data synthesis is to generate a joint posterior predictive distribution of $Y|X$ where $Y$ are variables to be synthesized and $X$ are unsynthesized variables. The synthetic data are generated by sampling new values from this distribution. In \SynLBD, variables are synthesized in a sequential fashion, with categorical variables being generally processed first using a variant of Dirichlet-Multinomial models. Continuous variables are then synthesized using a normal linear regression model with kernel density-based transformation \citep{WOODCOCK20094228}.\footnote{\textcite{RePEc:cen:wpaper:14-12} shift  to a Classification and Regression Trees (CART) model with Bayesian bootstrap. } The synthesis models are run independently for each industry. \SynLBD{} is implemented in SAS\texttrademark, which is frequently used in national statistical offices.

To evaluate whether synthetic data algorithms developed in the U.S. can be adapted to generate similar synthetic data for other countries, \textcite{RePEc:cen:wpaper:14-13} implement \SynLBD{} to the German Longitudinal Business Database (GLBD). In this paper, we extend the analysis from the earlier paper, and extend the application to the Canadian context (SynLEAP). 


\subsection{Harmonizing and Preprocessing}

In all countries, the underlying data provides annual measures. However, \SynLBD{} assumes a longitudinal (wide) structure of the dataset, with invariant industry (and location). In all cases, the modal industry is chosen to represent the entity's industrial activity. 

Further adjustments made to the \ac{BHP} for this project, include estimating full-year payroll, creating time-consistent geographic information, and applying employment flow methods \citep{RePEc:iab:iabfme:201006_en} to adjust for spurious births and deaths in establishment identifiers. \citet{SJIAOS-2014b} provide a detailed description of the steps taken to harmonize the input data. 

\subsection{Limitations}

In both cases, we encountered various technical and data-driven limitations. In all countries, the first year and last year is occasionally problematic, and both were dropped. Furthermore, in all countries, the synthesis of certain industries failed to complete. In both Canada and the US, this number is less than 10. In Canada, they account for about 7 percent of the total number of observations (see Table \ref{tab:Synthesized_observations} in the Appendix).

In the German case, our experiments were limited to only a handful of industries, due to a combination of time and software availability factors. The results should still be considered preliminary. In both countries, as outlined in Section~\ref{sec:data}, there are subtle but potentially important differences in the various variable definitions. Industry coding differs across all three countries, and the level of detail in each of the industry codings may affect the success and precision of the synthesis.\footnote{\textcite{StatisticsCanada1991}, when comparing the 1987 US \ac{SIC} to the 1980 Canadian \ac{SIC},  already pointed out that the degree of specialization, the organization of production, and the size of the respective markets differed. Thus, the density of establishments within each of the chosen categories is likely to affect the quality of the synthesis.} 

Furthermore, due to the nature of the underlying data, entities are establishments in Germany and the US, but employers in Canada. \SynLBD{} should work on any level of entity aggregation (see \citet{RePEc:cen:wpaper:14-12} for an application to hierarchical firm data with both firm/employer and establishment level imputation). However, it may yet again affect the observed density of the data within industry-year categories, and therefore the overall comparability. 

Both the German and the Canadian data experience some level of industry coding change, which may affect the classification of some entities. 

Finally, due to a feature of \SynLBD{} that we do not fully understand, the last year of the data generally was of poor quality. For some industry-country pairs, this also happened in the first year. We dropped those observations. 

\subsection{Measuring outcomes}

In order to assess the outcomes of the experiment, we inspect analytical validity by various measures and also evaluate the extent of confidentiality protection. To check analytical validity, we compare basic univariate time series between the synthetic and confidential data (employment, entity entry and exit rates, job creation and destruction rates), and the distribution of entities (firms and establishment, depending on country),  employment, and payroll across time by industry. For a more complex assessment, we compute a dynamic panel data model of economic (employment) growth on each dataset. We deliberately refrain from using the confidence interval overlap measure (CIO) proposed by \citet{Woo_Reiter_Oganian_Karr_2009} in all these evaluations. The CIO is a popular measure when evaluating the validity for specific analyses. It evaluates how much the confidence intervals of the original data and the protected data overlap. We feel that this measure is not useful in our context as most of our analyses are based on millions of records making the confidence intervals so small that the confidence intervals will never overlap even if the estimates between the original data and the synthetic data are very close. Besides, both datasets cover the entire population and depending on the interpretation of the results calculating a sampling error is not always meaningful. 

To provide a more comprehensive measure of  quality of the synthetic data relative to the confidential data, we compute the $pMSE$ \parencite[propensity score mean-squared error,][]{Woo_Reiter_Oganian_Karr_2009,SnokeSlavkovic2018,Snoke_RSSA2018}: the mean-squared error of the predicted probabilities (i.e., propensity scores) for those two databases. Specifically, $pMSE$ is a metric to assess how well we are able to discern the high distributional similarity between synthetic data and confidential data. 

We follow  \textcite{Woo_Reiter_Oganian_Karr_2009} and \textcite{SnokeSlavkovic2018} to calculate the $pMSE$, using the following algorithm:  
\begin{enumerate}
    \item Append the $n_1$ rows of the confidential database $X$ to the $n_2$ rows of the synthetic database $X^s$ to create $X^{comb}$ with $N=n_1 + n_2$ rows, where both $X$ and $X^s$ are in the long format.
    \item Create a variable $I_{et}$ denoting membership of an observation for entity $e$, $e=1,\ldots,E$, at time point $t$, $t=1,\ldots,T$, in the component databases,  $I_{et}=\{1: X^{comb}_{et} \in X^s\}$. $I_{et}$ takes on values of $1$ for the synthetic database and $0$ for the confidential database. 
    \item Fit the following model to predict $I$
    \begin{eqnarray}	
        P(I_{et}=1) & = &\beta_0 + \beta_{1} Emp_{et} + \beta_{2} Pay_{et} + Age_{et}^{T}\beta_{3} + \lambda_t + \alpha_i + \epsilon_{et}, \label{pMSE}
     \end{eqnarray}
         where $Emp_{et}$ is  log employment  of entity $e$ in year $t$, $Pay_{et}$ is  log payroll of entity $e$ in year $t$, $Age_{et}$ is a vector of age classes of entity $e$ in year $t$, $\lambda_t$ is a year fixed effect, $\alpha_i$ is an unobserved time-invariant industry-specific effect, and $\epsilon_{et}$ is the disturbance term of entity $e$ in year $t$. 
    \item Calculate the predicted probabilities, $\hat{p}_{et}$.
    \item Compute  $pMSE=\frac{1}{N}\sum_{t=1}^T\sum_{e=1}^E(\hat{p}_{et} - c)^2$, where $c=n_2/N$.
\end{enumerate}
If $n_1 = n_2$, $pMSE$ = 0 means every $\hat{p}_i = 0.5$, and the two databases are distributionally indistinguishable, suggesting  high analytical validity. Since the pMSE depends on the number of predictors included in the propensity score model, \textcite{Snoke_RSSA2018} derived the expected value and standard deviation for the $pMSE$ under the null hypothesis that the synthesis model is correct, i.e, it matches the true data generating process \parencite[Equation 1]{Snoke_RSSA2018}:

$$
E[pMSE] = (k-1)(1-c)^2 \frac{c}{N}
$$
and
$$
StDev[pMSE] = \sqrt{2(k-1)}(1-c)^2 \frac{c}{N}
$$
where $k$ is the number of synthesized variables used in the propensity model. To measure the analytical validity of the synthetic data, they suggest to look at the \textit{pMSE ratio} and the \textit{standardized pMSE}. The $pMSE$ ratio is computed by dividing the estimated $pMSE$ by its expectation under the null. The standardized $pMSE$ is computed as $(pMSE-E[pMSE])/StDev[pMSE]$. Under the null hypothesis, the $pMSE$ ratio has an expectation of 1 and the expectation of the standardized $pMSE$ is zero.

% We then assess fit of the model in two ways. \todo{What is the other assessment you had in mind?} First, analytic validity --- statistical precision --- can be assessed using confidence interval overlap measures   \citep{tas2006}. \todo{Do we compute them? If not, we should drop this. If we want we could include a short discussion why CIO is not a helpful measure here}
%We compute the \emph{interval overlap measure} $J_{k,m}$ for parameter $k$ in model $m$. Consider the overlap of confidence intervals $(L,U)$ for $\beta_{k,m}$ (estimated from the confidential data) and $(L^{*},U^{*})$ for $\beta_{k,m}^*$ (from the synthetic data). Let $L^{over} = \max (L,L^{*} )$ and $U^{over} = \min (U,U^{*})$. Then the confidence interval  overlap  is computed as
%$$
%J_{k,m}^{*} = \frac{1}{2} \left [ \frac{U^{over} - L^{over}}{U-L} + \frac{U^{over} - L^{over}}{U^*-L ^*}        \right ]
%$$
%We summarize  the $J_{k,m}$ in a variety of ways (mean, median, max).